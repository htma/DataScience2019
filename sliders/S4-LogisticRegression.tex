% Python Programming S4: Logistic Regression 
% created on Aug 25, 2019

\documentclass[14 pt]{beamer}
\usetheme[
	bullet=circle,		% Other option: square
	bigpagenumber,		% circled page number on lower right
	topline=true,			% colored bar at the top of the frame 
	shadow=false,			% Shading for beamer blocks
	]{Flip}

\definecolor{UBCblue}{rgb}{0.04706, 0.13725, 0.26667} % UBC Blue (primary)
\usecolortheme[named=UBCblue]{structure}
\setbeamertemplate{frametitle}{\color{UBCblue}\bfseries\insertframetitle\par\vskip-6pt\hrulefill}

%\usecolortheme[named=Mahogany]{structure} % Sample dvipsnames color

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}

\graphicspath{{images/}}	% Put all images in this directory. Avoids clutter.

\usetikzlibrary{backgrounds}
\usetikzlibrary{mindmap,trees}	% For mind map

% increase space between items
\let\olditem\item
\renewcommand{\item}{\olditem\vspace{4pt}}

\newcommand{\comment}[1]{\textcolor{comment}{\footnotesize{#1}\normalsize}} % comment mild
\newcommand{\Comment}[1]{\textcolor{Comment}{\footnotesize{#1}\normalsize}} % comment bold
\newcommand{\COMMENT}[1]{\textcolor{COMMENT}{\footnotesize{#1}\normalsize}} % comment crazy bold
\newcommand{\Alert}[1]{\textcolor{Alert}{#1}} % louder alert
\newcommand{\ALERT}[1]{\textcolor{ALERT}{#1}} % loudest alert

\tikzstyle{every picture}+=[remember picture]

\author[mht]{CS}
\title[Deep Learning with PyTorch]{Deep Learning with PyTorch}
\institute{Northeastern University at Qinhuangdao}

\begin{document}
\begin{frame}[c]
\begin{center}
	\textcolor{normal text.fg!50!Comment}{\textbf{\Large{Deep Learning with PyTorch}}}
	\vspace{4em}

    \COMMENT{\large{Lecture 4: Logistic Regression}} \\
\vspace{4em}
    \Comment{{Ma Haitao}} \\
\comment{\textit{Northeastern University at Qinhuangdao}}\\
\end{center}
\end{frame}

\begin{frame}{Outline}
  \begin{itemize}
  \item Overview Logistic Reguression 
  \item Problem: Prediction
  \item Pytorch Implementation
  \end{itemize}
\end{frame}

\begin{frame}{Logistic Regression}
   Classification algorithm
  \begin{itemize}
  \item Example: Spam vs No Spam
    \begin{itemize}
    \item  Input: Bunch of words
    \item Output: Probability spam or not
  \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
  \frametitle{ vs. Linear Regression}
   Linear regression:
  \begin{itemize}
  \item Output: numeric value given inputs
  \end{itemize}
  Logistic regression:
  \begin{itemize}
  \item Output: probability $[0, 1]$ given input belonging to a class
  \end{itemize}
\end{frame}

\begin{frame}
  \begin{center}
      \Large{Logistic Functions}
    \end{center}
\end{frame}

\begin{frame}
  \frametitle{Logistic Function $\sigma()$}
  Two-class logistic regression
  \begin{displaymath}
    \mathbf{y} = A\mathbf{x} + b
  \end{displaymath}
  \begin{itemize}
  \item Where $\mathbf{y}$ is a vector comprising the 2-class prediction $y_0$ and
    $y_1$
  \item Where the labels are $y_0 = 0$ and $y_1 = 1$
  \item $\sigma(y_1)=\frac{1}{1-e^{y_1}}$ : Estimated probability that $y = 1$
    
  \item $\sigma(y_0) = 1 - \sigma(y_1)$: Estimated probability that $y = 0$
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Softmax Function $\sigma()$}
  Multi-class logistic regression
  \begin{displaymath}
    \mathbf{y} = A\mathbf{x} + b
  \end{displaymath}
  \begin{itemize}
  \item Where $\mathbf{y}$ is a vector comprising the 4-class $K = 4$ prediction
    $y_0$, $y_1$, $y_2$  and $y_3$
    $y_1$
  \item Where the 4 labels are $y_0 = 0$ , $y_1 = 1$, $y_2 = 2$ and $y_3 =
    3$
  \item $\sigma(y_i) = \frac{e^{y_i}}{\sum_i^K e^{y_i}}$, where $K=4$
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Softmax Example: $K=4$}
  \begin{displaymath}
    \sigma(y_i) = \frac{e^{y_i}}{\sum_i^K e^{y_i}}
  \end{displaymath}
  \begin{itemize}
  \item $y_0 = 1.3$, $y_1 = 1.2$, $y_2 = 4.5$, $y_3 = 4.8$
  \item $\sigma(y_0) = \frac{e^{1.3}}{e^{1.3}+e^{1.2}+e^{4.5}+e^{4.8}} = 0.017$
  \item $\sigma(y_1) = \frac{e^{1.2}}{e^{1.3}+e^{1.2}+e^{4.5}+e^{4.8}} = 0.015$
  \item $\sigma(y_2) = \frac{e^{4.5}}{e^{1.3}+e^{1.2}+e^{4.5}+e^{4.8}} = 0.412$
  \item $\sigma(y_3) = \frac{e^{4.8}}{e^{1.3}+e^{1.2}+e^{4.5}+e^{4.8}} = 0.556$
  \item $\sigma(y_0)+\sigma(y_1)+\sigma(y_2)+\sigma(y_3) = 1$
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Cross Entropy Function D() for 2 Class}
  \begin{itemize}
  \item $S$  is our softmax outputs and $L$ are our labels
  \item $D(S, L) = -(L\log S + (1 - L)\log (1 - S))$
    \begin{itemize}
    \item If $L = 0$ : $D(S, 0)  = -\log(1 - S)$
      \begin{itemize}
      \item $-\log (1 - S)$: less positive if $S \to 0$
      \item $-\log (1 - S)$: more positive if $S \to 1$
      \end{itemize}
    \item If $L = 1$: $D(S, 1) = -\log S$
      \begin{itemize}
      \item $-\log S$: less positive if $S\to 1$
      \item $-\log S$: more positive if $S\to 0$
      \end{itemize}
    \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Cross Entropy Function D() for More Than 2 Class}
  \begin{displaymath}
    D(S, L) = -\sum_1^K L_i \log (S_i)
  \end{displaymath}
  \begin{itemize}
  \item $K$: number of classes
  \item $L_i$: label of $i$-th class, 1 if that's the class else 0
  \item $S_i$: output of softmax for $i$-th class
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Cross Entropy Loss over $N$ samples}
  \begin{itemize}
  \item Goal: Minimizing Cross Entropy Loss, $L$
  \item $Loss = \frac{1}{N}\sum_j^N D_j$
    \begin{itemize}
    \item $D_j$: $j$-th sample of cross entropy function $D(S, L)$
    \item $N$: number of samples
    \item $Loss$: average cross entropy loss over $N$ samples
    \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}
  \begin{center}
    \Large{Building a Logistic Regression Model}
  \end{center}
\end{frame}

\begin{frame}
  \frametitle{A Logistic Regression Model}
  \begin{figure}
    \centering
    \includegraphics[width=\textwidth]{lr-model.png}
  \end{figure}
\end{frame}

\begin{frame}
  \frametitle{Steps}
     \setbeamertemplate{enumerate items}[circle]
  \begin{enumerate}
  \item Load Dataset
  \item Create Model Class
  \item Instantiate Model 
  \item Instantiate Loss and Optimizer 
  \item Train Model
  \end{enumerate}
\end{frame}

\begin{frame}[fragile]
  \frametitle{1a: Loading MNIST Train Dataset}

  \begin{block}{}
\begin{verbatim}
import torch
import torch.nn as nn
import torchvision.transforms as transforms
import torchvision.datasets as datasets

train_dataset = datasets.MNIST(
            root='./data', 
            train=True, 
            transform=transforms.ToTensor(),
            download=True)

print(len(train_dataset))
\end{verbatim}
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Inspecting a Single Image}
\small{
  \begin{block}{}
\begin{verbatim}
print(train_dataset[0][0].size()) 
-> torch.Size([1,28,28])
print(train_dataset[0][1]) 
-> tensor(5)

import matplotlib.pyplot as plt
import numpy as np

show_img = train_dataset[0][0].numpy().reshape(28,28)
plt.imshow(show_img, cmap='gray')
plt.show()
\end{verbatim}
  \end{block}
}
\end{frame}
\begin{frame}
  \frametitle{Show the Image}
  \begin{figure}
    \centering
    \includegraphics[width=\textwidth]{Five.png}
  \end{figure}
\end{frame}

\begin{frame}[fragile]
  \frametitle{1b: Loading MNIST Test Dataset}
  \begin{block}{}
\begin{verbatim}
test_dataset = datasets.MNIST(root='./data', 
           train=False, 
           transform=transforms.ToTensor())

print(len(test_dataset))
\end{verbatim}
  \end{block}
\end{frame}

\begin{frame}
  \frametitle{2: Make Dataset Iterable}
  \begin{itemize}
  \item mini-batch: batch size means the model would look at how many
    images before updating the model's weights
  \item iteration: 1 mini-batch forward \& backward pass
  \item epoch: 1 epoch  running through the whole dataset once
  \end{itemize}
\end{frame}

\begin{frame}[fragile]{Defining Epochs}
\small{
  \begin{block}{}
\begin{verbatim}
batch_size = 100
n_iters = 3000
num_epochs = n_iters/(len(train_dataset)/batch_size)
num_epochs = int(num_epochs)
print(num_epochs)

train_loader = torch.utils.data.DataLoader(
               dataset=train_dataset, 
               batch_size=batch_size, 
               shuffle=True)

test_loader = torch.utils.data.DataLoader(
              dataset=test_dataset, 
              batch_size=batch_size, 
              shuffle=False)
\end{verbatim}
  \end{block}
}
\end{frame}

\begin{frame}[fragile]
  \frametitle{3: Building Model}
\small{
  \begin{block}{}
\begin{verbatim}
class LogisticRegressionModel(nn.Module):
  def __init__(self, D_in, D_out):
    super(LogisticRegressionModel, self).__init__()
    self.linear = nn.Linear(D_in, D_out)

  def forward(self, x):
    out = self.linear(x)
    return out

D_in, D_out = 28*28, 10
model = LogisticRegressionModel(D_in, D_out)
print(model)
\end{verbatim}
  \end{block}
}
\end{frame}

\begin{frame}[fragile]
  \frametitle{4: Instantiate Loss and Optimizer}
  \begin{block}{Instantiate Loss}
    criterion = nn.CrossEntropyLoss()  
  \end{block}
     \setbeamertemplate{enumerate items}[circle]
  \begin{enumerate}
  \item Computes softmax (logistic/softmax function) 
  \item Computes cross entropy
  \end{enumerate}

  \begin{block}{Instantiate optimizer}
\small{
\begin{verbatim}
learning_rate = 0.001
optimizer = torch.optim.SGD(model.parameters(), 
                            lr=learning_rate)  
\end{verbatim}
}
  \end{block}
\end{frame}

\begin{frame}
  \frametitle{Cross Entropy Loss}
  \begin{figure}
    \centering
    \includegraphics[width=\textwidth]{cross_entropy.png}
  \end{figure}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Parameters In-Depth}
  \begin{block}{}
\begin{verbatim}
print(model.parameters())

print(len(list(model.parameters())))

print(list(model.parameters())[0].size())
-> torch.Size([10, 784])
print(list(model.parameters())[1].size())
-> torch.Size([10])
\end{verbatim}
  \end{block}
\end{frame}

\begin{frame}
  \frametitle{Train Model}
  \begin{itemize}
  \item Convert inputs/labels to tensors with gradients
  \item Clear gradient buffets
  \item Get output given inputs
  \item Get loss
  \item Get gradients w.r.t. parameters
  \item Update parameters using gradients
  \item REPEAT
  \end{itemize}
\end{frame}
\end{document}
%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
